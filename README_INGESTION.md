# Guide d'Ingestion : `python -m ingestion.ingest --clean`

Ce guide explique en dÃ©tail ce que fait la commande d'ingestion avec l'option `--clean` dans le systÃ¨me Agentic RAG avec Graphe de Connaissances.

## Vue d'ensemble

La commande `python -m ingestion.ingest --clean` effectue un processus complet d'ingestion de documents avec nettoyage prÃ©alable des bases de donnÃ©es. Cette commande est essentielle pour prÃ©parer le systÃ¨me avant d'utiliser l'agent IA.

## Processus dÃ©taillÃ© Ã©tape par Ã©tape

### Ã‰tape 1 : Nettoyage des bases de donnÃ©es (--clean)
```
ğŸ§¹ NETTOYAGE DES DONNÃ‰ES EXISTANTES
â”œâ”€â”€ PostgreSQL
â”‚   â”œâ”€â”€ DELETE FROM messages          # Suppression des conversations
â”‚   â”œâ”€â”€ DELETE FROM sessions          # Suppression des sessions utilisateur
â”‚   â”œâ”€â”€ DELETE FROM chunks            # Suppression des fragments de documents
â”‚   â””â”€â”€ DELETE FROM documents         # Suppression des documents
â””â”€â”€ Neo4j (Graphe de connaissances)
    â””â”€â”€ CLEAR GRAPH                   # Suppression complÃ¨te du graphe Graphiti
```

**âš ï¸ ATTENTION** : Cette Ã©tape supprime **toutes** les donnÃ©es existantes des deux bases de donnÃ©es !

### Ã‰tape 2 : Initialisation du pipeline
```
ğŸ”§ INITIALISATION DU SYSTÃˆME
â”œâ”€â”€ ğŸ”Œ Connexion Ã  PostgreSQL (DATABASE_URL)
â”œâ”€â”€ ğŸ”Œ Connexion Ã  Neo4j (NEO4J_URI)
â”œâ”€â”€ ğŸ¤– Initialisation du modÃ¨le LLM pour l'ingestion
â”œâ”€â”€ ğŸ“Š Initialisation du client d'embeddings
â”œâ”€â”€ âœ‚ï¸  Initialisation du chunker sÃ©mantique
â””â”€â”€ ğŸ•¸ï¸  Initialisation du constructeur de graphe Graphiti
```

### Ã‰tape 3 : DÃ©couverte des documents
```
ğŸ“ SCAN DU DOSSIER DOCUMENTS
â”œâ”€â”€ ğŸ“‚ Parcours du dossier 'documents/' (par dÃ©faut)
â”œâ”€â”€ ğŸ” Recherche des fichiers *.md et *.markdown
â”œâ”€â”€ ğŸ“‹ CrÃ©ation de la liste des fichiers Ã  traiter
â””â”€â”€ ğŸ“Š Affichage : "Found X markdown files to process"
```

### Ã‰tape 4 : Traitement de chaque document

Pour chaque fichier markdown trouvÃ©, le systÃ¨me effectue :

#### 4.1 Lecture et analyse initiale
```
ğŸ“– LECTURE DU DOCUMENT
â”œâ”€â”€ ğŸ“„ Lecture du contenu (UTF-8, fallback latin-1)
â”œâ”€â”€ ğŸ·ï¸  Extraction du titre
â”‚   â”œâ”€â”€ Recherche du premier # dans les 10 premiÃ¨res lignes
â”‚   â””â”€â”€ Fallback : nom du fichier sans extension
â”œâ”€â”€ ğŸ“Š Extraction des mÃ©tadonnÃ©es
â”‚   â”œâ”€â”€ Parsing du YAML frontmatter (si prÃ©sent)
â”‚   â”œâ”€â”€ Calcul de la taille du fichier
â”‚   â”œâ”€â”€ Comptage des lignes et mots
â”‚   â””â”€â”€ Date d'ingestion
â””â”€â”€ ğŸ“ PrÃ©paration pour le chunking
```

#### 4.2 DÃ©coupage sÃ©mantique intelligent
```
âœ‚ï¸  CHUNKING SÃ‰MANTIQUE
â”œâ”€â”€ ğŸ—ï¸  Division structurelle
â”‚   â”œâ”€â”€ Headers markdown (# ## ###)
â”‚   â”œâ”€â”€ Paragraphes (lignes vides multiples)
â”‚   â”œâ”€â”€ Listes (- * + 1. 2. 3.)
â”‚   â”œâ”€â”€ Blocs de code (```)
â”‚   â””â”€â”€ Tableaux (|)
â”œâ”€â”€ ğŸ§  Regroupement sÃ©mantique
â”‚   â”œâ”€â”€ Analyse de cohÃ©rence par le LLM
â”‚   â”œâ”€â”€ Respect de la taille max (1000 chars par dÃ©faut)
â”‚   â”œâ”€â”€ Chevauchement entre chunks (200 chars)
â”‚   â””â”€â”€ PrÃ©servation du contexte
â””â”€â”€ ğŸ“Š CrÃ©ation des objets DocumentChunk
    â”œâ”€â”€ Index du chunk
    â”œâ”€â”€ Position dans le document
    â”œâ”€â”€ MÃ©tadonnÃ©es enrichies
    â””â”€â”€ Estimation du nombre de tokens
```

#### 4.3 GÃ©nÃ©ration des embeddings
```
ğŸ§® GÃ‰NÃ‰RATION DES EMBEDDINGS VECTORIELS
â”œâ”€â”€ ğŸ”¤ Tokenisation du contenu de chaque chunk
â”œâ”€â”€ ğŸŒ Appel Ã  l'API d'embeddings (OpenAI/Ollama/etc.)
â”œâ”€â”€ ğŸ“Š GÃ©nÃ©ration du vecteur (1536 dimensions pour OpenAI)
â””â”€â”€ ğŸ”¢ Conversion au format PostgreSQL vector
```

#### 4.4 Sauvegarde en PostgreSQL
```
ğŸ’¾ SAUVEGARDE EN BASE VECTORIELLE
â”œâ”€â”€ ğŸ“„ INSERT INTO documents
â”‚   â”œâ”€â”€ title, source, content
â”‚   â”œâ”€â”€ metadata (JSON)
â”‚   â””â”€â”€ timestamps
â””â”€â”€ ğŸ§© INSERT INTO chunks (pour chaque chunk)
    â”œâ”€â”€ document_id (rÃ©fÃ©rence)
    â”œâ”€â”€ content (texte)
    â”œâ”€â”€ embedding (vecteur)
    â”œâ”€â”€ chunk_index
    â”œâ”€â”€ metadata (JSON)
    â””â”€â”€ token_count
```

#### 4.5 Construction du graphe de connaissances
```
ğŸ•¸ï¸  CONSTRUCTION DU GRAPHE GRAPHITI
â”œâ”€â”€ ğŸ“ PrÃ©paration des Ã©pisodes
â”‚   â”œâ”€â”€ CrÃ©ation d'un episode_id unique
â”‚   â”œâ”€â”€ Limitation du contenu (6000 chars max)
â”‚   â”œâ”€â”€ Troncature intelligente aux limites de phrases
â”‚   â””â”€â”€ Ajout du contexte minimal
â”œâ”€â”€ ğŸ¤– Traitement par Graphiti
â”‚   â”œâ”€â”€ Extraction automatique d'entitÃ©s
â”‚   â”‚   â”œâ”€â”€ Entreprises (Google, Microsoft, OpenAI...)
â”‚   â”‚   â”œâ”€â”€ Personnes (Sam Altman, Satya Nadella...)
â”‚   â”‚   â”œâ”€â”€ Technologies (GPT, Claude, LLaMA...)
â”‚   â”‚   â””â”€â”€ Lieux et organisations
â”‚   â”œâ”€â”€ Identification des relations
â”‚   â”‚   â”œâ”€â”€ Partenariats (Microsoft â†” OpenAI)
â”‚   â”‚   â”œâ”€â”€ Acquisitions (Google â†’ DeepMind)
â”‚   â”‚   â”œâ”€â”€ Concurrence (OpenAI vs Anthropic)
â”‚   â”‚   â””â”€â”€ Collaborations
â”‚   â””â”€â”€ CrÃ©ation des nÅ“uds et arÃªtes temporels
â”œâ”€â”€ â±ï¸  DÃ©lai entre Ã©pisodes (0.5s pour Ã©viter la surcharge)
â””â”€â”€ ğŸ“Š Comptage des Ã©pisodes crÃ©Ã©s
```

### Ã‰tape 5 : RÃ©sumÃ© et statistiques finales
```
ğŸ“Š RAPPORT D'INGESTION FINAL
â”œâ”€â”€ ğŸ“ˆ Statistiques globales
â”‚   â”œâ”€â”€ Nombre de documents traitÃ©s
â”‚   â”œâ”€â”€ Nombre total de chunks crÃ©Ã©s
â”‚   â”œâ”€â”€ Nombre d'entitÃ©s extraites
â”‚   â”œâ”€â”€ Nombre d'Ã©pisodes de graphe crÃ©Ã©s
â”‚   â”œâ”€â”€ Nombre d'erreurs rencontrÃ©es
â”‚   â””â”€â”€ Temps de traitement total
â”œâ”€â”€ ğŸ“‹ DÃ©tail par document
â”‚   â”œâ”€â”€ âœ“ SuccÃ¨s : titre, chunks, entitÃ©s
â”‚   â””â”€â”€ âœ— Ã‰checs : titre, erreurs dÃ©taillÃ©es
â””â”€â”€ ğŸ Fermeture des connexions
    â”œâ”€â”€ Fermeture du pool PostgreSQL
    â”œâ”€â”€ Fermeture du client Neo4j
    â””â”€â”€ Nettoyage des ressources
```

## Options de la commande

| Option | Description | Valeur par dÃ©faut |
|--------|-------------|-------------------|
| `--clean` | Nettoie les donnÃ©es existantes avant ingestion | `False` |
| `--documents` | Dossier contenant les documents markdown | `"documents"` |
| `--chunk-size` | Taille maximale des chunks en caractÃ¨res | `1000` |
| `--chunk-overlap` | Chevauchement entre chunks en caractÃ¨res | `200` |
| `--no-semantic` | DÃ©sactive le dÃ©coupage sÃ©mantique (utilise le dÃ©coupage simple) | `False` |
| `--no-entities` | DÃ©sactive l'extraction d'entitÃ©s | `False` |
| `--fast` | Mode rapide : ignore la construction du graphe | `False` |
| `--verbose` | Active les logs dÃ©taillÃ©s | `False` |

## Exemples d'utilisation

### Ingestion complÃ¨te avec nettoyage (recommandÃ©)
```bash
python -m ingestion.ingest --clean --verbose
```

### Ingestion rapide sans graphe de connaissances
```bash
python -m ingestion.ingest --clean --fast
```

### Ingestion avec paramÃ¨tres personnalisÃ©s
```bash
python -m ingestion.ingest --clean --chunk-size 800 --chunk-overlap 150
```

### Ajout de nouveaux documents sans nettoyage
```bash
python -m ingestion.ingest --documents new_docs/
```

## Exemple de sortie complÃ¨te

```
2024-01-15 10:30:15,123 - ingestion.ingest - INFO - Initializing ingestion pipeline...
2024-01-15 10:30:16,456 - ingestion.ingest - WARNING - Cleaning existing data from databases...
2024-01-15 10:30:17,789 - ingestion.ingest - INFO - Cleaned PostgreSQL database
2024-01-15 10:30:18,012 - ingestion.ingest - INFO - Cleaned knowledge graph
2024-01-15 10:30:19,345 - ingestion.ingest - INFO - Ingestion pipeline initialized
2024-01-15 10:30:20,678 - ingestion.ingest - INFO - Found 5 markdown files to process
2024-01-15 10:30:21,901 - ingestion.ingest - INFO - Processing file 1/5: documents/openai_funding.md
2024-01-15 10:30:22,234 - ingestion.ingest - INFO - Processing document: OpenAI Funding Analysis
2024-01-15 10:30:35,567 - ingestion.graph_builder - INFO - Adding 8 chunks to knowledge graph for document: OpenAI Funding Analysis
2024-01-15 10:30:45,890 - ingestion.graph_builder - INFO - âœ“ Added episode openai_funding.md_0_1705312845.123 to knowledge graph (1/8)
2024-01-15 10:30:46,890 - ingestion.graph_builder - INFO - âœ“ Added episode openai_funding.md_1_1705312846.456 to knowledge graph (2/8)
...
Progress: 1/5 documents processed
2024-01-15 10:32:15,234 - ingestion.ingest - INFO - Processing file 2/5: documents/microsoft_openai.md
...
Progress: 5/5 documents processed

==================================================
INGESTION SUMMARY
==================================================
Documents processed: 5
Total chunks created: 42
Total entities extracted: 156
Total graph episodes: 42
Total errors: 0
Total processing time: 180.45 seconds

âœ“ OpenAI Funding Analysis: 8 chunks, 31 entities
âœ“ Microsoft-OpenAI Partnership: 12 chunks, 45 entities
âœ“ Google AI Strategy: 7 chunks, 28 entities
âœ“ Meta AI Developments: 9 chunks, 34 entities
âœ“ Amazon AI Initiatives: 6 chunks, 18 entities
```

## ConsidÃ©rations importantes

### âš ï¸ Temps de traitement
- **Petits documents** (1-5 docs) : 2-5 minutes
- **Corpus moyen** (10-20 docs) : 10-30 minutes  
- **Gros corpus** (50+ docs) : 1+ heure
- Le graphe de connaissances est le plus coÃ»teux en temps

### ğŸ’¾ Utilisation des ressources
- **RAM** : ~200-500 MB pendant l'ingestion
- **CPU** : Intensif pendant l'extraction d'entitÃ©s
- **RÃ©seau** : Appels API pour LLM et embeddings
- **Stockage** : ~10-50 MB par document selon la taille

### ğŸ”‘ APIs requises
- **LLM API** : Pour le chunking sÃ©mantique et l'extraction d'entitÃ©s
- **Embeddings API** : Pour la gÃ©nÃ©ration des vecteurs
- **Bases de donnÃ©es** : PostgreSQL et Neo4j doivent Ãªtre accessibles

### ğŸš¨ Gestion des erreurs
- **Timeout API** : Retry automatique avec backoff
- **Chunks trop grands** : Troncature automatique
- **Erreurs de parsing** : Document ignorÃ©, traitement continue
- **Ã‰chec de connexion** : ArrÃªt propre avec nettoyage

## DÃ©pannage

### ProblÃ¨mes courants

**"No markdown files found"**
```bash
# VÃ©rifiez le dossier documents
ls -la documents/
# Ou spÃ©cifiez un autre dossier
python -m ingestion.ingest --documents mon_dossier/
```

**"Database connection failed"**
```bash
# Testez votre connexion PostgreSQL
psql "$DATABASE_URL" -c "SELECT 1;"
```

**"Neo4j connection failed"**
```bash
# VÃ©rifiez que Neo4j est dÃ©marrÃ©
curl -u neo4j:password http://localhost:7474/
```

**"API rate limit exceeded"**
```bash
# Utilisez des modÃ¨les locaux ou attendez
# Ou configurez un autre provider dans .env
```

**Chunks trop volumineux pour Graphiti**
```bash
# RÃ©duisez la taille des chunks
python -m ingestion.ingest --clean --chunk-size 600
```

---

**ğŸ“ Note** : Ce processus d'ingestion est la base de tout le systÃ¨me. Une fois terminÃ©, votre agent IA pourra utiliser Ã  la fois la recherche vectorielle et le graphe de connaissances pour rÃ©pondre aux questions de maniÃ¨re intelligente. 